pretrained_model_name_or_path: Qwen/Qwen-Image
data_config:
  train_batch_size: 1  # Higher batch size for high-end GPU
  num_workers: 8
  img_size: 1024
  caption_dropout_rate: 0.05  # Lower dropout for Korean text images
  img_dir: ./korean_dataset
  random_ratio: false # Keep consistent aspect ratios for text signs
  caption_type: txt
report_to: null
train_batch_size: 1
output_dir: ./output/korean_1
max_train_steps: 12000  # Fewer steps with higher batch size
learning_rate: 1e-4  # Slightly lower learning rate for larger batch
lr_scheduler: constant
lr_warmup_steps: 10
adam_beta1: 0.9
adam_beta2: 0.999
adam_weight_decay: 0.01
adam_epsilon: 1e-8
max_grad_norm: 1.0
logging_dir: logs
mixed_precision: "bf16"
checkpointing_steps: 300
checkpoints_total_limit: 20
tracker_project_name: korean_text_lora
resume_from_checkpoint: latest
gradient_accumulation_steps: 4  # Effective batch size of 8
rank: 64  # Higher rank for high-end training
# precompute_text_embeddings: false  # No precompute for high-end GPU
# precompute_image_embeddings: false
# quantize: false  # No quantization for high-end GPU
# adam8bit: false
# save_cache_on_disk: false
